{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Toxic Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load required packages\n",
    "import nltk\n",
    "import math\n",
    "import decimal\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from nltk.corpus import reuters\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.stem.porter import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_data = pd.read_csv(\"E:/GW/Textbook/Natural Language Processing/project/jigsaw-unintended-bias-in-toxicity-classification/train.csv\")\n",
    "train_set = whole_data[0:500000] # Train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "250\n",
      "250\n"
     ]
    }
   ],
   "source": [
    "# Obtain the validation set\n",
    "corpus = whole_data[1000000:1006230] \n",
    "validation_set = []\n",
    "numb1 = 0\n",
    "numb0 = 0\n",
    "for row in corpus.itertuples():\n",
    "    if getattr(row, 'target') >=0.7:\n",
    "        numb1 +=1 \n",
    "        #print(getattr(row, 'target'))\n",
    "        validation_set += [(getattr(row, 'comment_text'), math.ceil(getattr(row, 'target')))]\n",
    "for row in corpus.itertuples():    \n",
    "    if   numb0 <numb1:\n",
    "        if getattr(row, 'target') ==0:\n",
    "            #print(getattr(row, 'target'))\n",
    "            validation_set += [(getattr(row, 'comment_text'), math.floor(getattr(row, 'target')))]\n",
    "            numb0 +=1\n",
    "        \n",
    "print(len(validation_set))\n",
    "print(numb1)\n",
    "print(numb0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "250\n",
      "250\n"
     ]
    }
   ],
   "source": [
    "# Obtain the test set\n",
    "corpus = whole_data[1500000:1506840] \n",
    "test_set = []\n",
    "numb1 = 0\n",
    "numb0 = 0\n",
    "for row in corpus.itertuples():\n",
    "    if getattr(row, 'target') >=0.7:\n",
    "        numb1 +=1 \n",
    "        #print(getattr(row, 'target'))\n",
    "        test_set += [(getattr(row, 'comment_text'), math.ceil(getattr(row, 'target')))]\n",
    "for row in corpus.itertuples():    \n",
    "    if   numb0 <numb1:\n",
    "        if getattr(row, 'target') ==0:\n",
    "            #print(getattr(row, 'target'))\n",
    "            test_set += [(getattr(row, 'comment_text'), math.floor(getattr(row, 'target')))]\n",
    "            numb0 +=1\n",
    "        \n",
    "print(len(test_set))\n",
    "print(numb1)\n",
    "print(numb0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Application\n",
    "#### 2.3.1 The first approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = train_set[0:50000]# Get 10 sub-groups of train set\n",
    "# like [0:50000], [50001:100000],[100001:150000].... [450001:500000]\n",
    "group = []\n",
    "numb1 = 0\n",
    "numb0 = 0\n",
    "for row in corpus.itertuples():\n",
    "    if getattr(row, 'target') >=0.7:\n",
    "        numb1 +=1 \n",
    "        #print(getattr(row, 'target'))\n",
    "        group += [(getattr(row, 'comment_text'), math.ceil(getattr(row, 'target')))]\n",
    "for row in corpus.itertuples():    \n",
    "    if   numb0 <numb1:\n",
    "        if getattr(row, 'target') ==0:\n",
    "            #print(getattr(row, 'target'))\n",
    "            group += [(getattr(row, 'comment_text'), math.floor(getattr(row, 'target')))]\n",
    "            numb0 +=1\n",
    "        \n",
    "print(len(group))\n",
    "print(numb1)\n",
    "print(numb0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####  step 1 Take the difference between tokens of sub-train set(group) and clean corpus to get potential toxic tokens.\n",
    "# The function to detect the noise tokens\n",
    "min_token_length = 2\n",
    "punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n",
    "def isNoise(token):     \n",
    "    is_noise = False\n",
    "    if porter_stemmer.stem(token.lower())in stopWords:\n",
    "        is_noise = True\n",
    "    elif len(token) < min_token_length:\n",
    "        is_noise = True\n",
    "    elif re.findall(r\"\\`|\\'|/\", token):\n",
    "        is_noise = True\n",
    "    elif re.findall(r\"[\\d]\",token):\n",
    "        is_noise = True  \n",
    "    elif re.findall(r\"^\\.\",token):\n",
    "        is_noise = True  \n",
    "    elif re.findall(r\"^\\.\",token):\n",
    "        is_noise = True  \n",
    "    elif not re.findall(r\"[A-Za-z0-9]\",token):\n",
    "        is_noise = True\n",
    "    elif re.findall(r\"\\…\",token):\n",
    "        is_noise = True\n",
    "    return is_noise \n",
    "\n",
    "# Tokenization preparation\n",
    "treebank_tokenizer = TreebankWordTokenizer()\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "stopWords = set(stopwords.words('english'))\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "# The functio to load Reuters data in NLTK\n",
    "def readdoc(data):\n",
    "    doc_texts = \"\"\n",
    "    for doc in data: \n",
    "        doc_texts += reuters.open(doc).read()\n",
    "    return doc_texts\n",
    "\n",
    "clean_corpus = readdoc(reuters.fileids()) # Load reuters data\n",
    "clean_tokens = treebank_tokenizer.tokenize(clean_corpus) # Word tokenization\n",
    "clean_corpus_tokens_d  = [porter_stemmer.stem(word.lower()) for word in clean_tokens]# Lematization and filter out noise tokens\n",
    "clean_corpus_tokens = set(clean_corpus_tokens_d) # Delete the duplicated tokens\n",
    "print(len(clean_corpus_tokens)) # Show the length of the clean corpus \n",
    "\n",
    "# Tokens of each of the 10 groups of train_set\n",
    "tokens=set(word.lower() for words in group for word in word_tokenize(words[0]) if not isNoise(word))\n",
    "print(len(tokens))\n",
    "\n",
    "# potential toxic tokens\n",
    "potential_toxic_tokens= [word for word in tokens if porter_stemmer.stem(word.strip(r\"^[\\-]+\\.\")) not in clean_corpus_tokens]\n",
    "print(len(set(potential_toxic_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####  step 2 Apply Naïve Bayes and get the informative tokens of each  of  these 10  groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the Naive Bayes\n",
    "train_w = [({word: (word in word_tokenize(x[0])) for word in set(potential_toxic_tokens)}, x[1]) for x in group] \n",
    "model = nltk.NaiveBayesClassifier.train(train_w)\n",
    "\n",
    "# The function to output the informative tokens\n",
    "def show_most_informative_features_in_listtest(classifier, n=10):\n",
    "    \"\"\"\n",
    "    Return a nested list of the \"most informative\" features \n",
    "    used by the classifier along with it's predominant labels\n",
    "    \"\"\"\n",
    "    cpdist = classifier._feature_probdist       # probability distribution for feature values given labels\n",
    "    feature_list = []\n",
    "    for (fname, fval) in classifier.most_informative_features(n):\n",
    "        def labelprob(l):\n",
    "            return cpdist[l, fname].prob(fval)\n",
    "        labels = sorted([l for l in classifier._labels if fval in cpdist[l, fname].samples()], \n",
    "                        key=labelprob)\n",
    "        if (cpdist[labels[-1], fname].prob(fval) != cpdist[labels[0], fname].prob(fval)) :\n",
    "            if labels[-1] ==1 & fval == True:\n",
    "                feature_list.append(fname)\n",
    "            if  fval == False:\n",
    "                if labels[-1] ==0:\n",
    "                    feature_list.append(fname)\n",
    "    return feature_list\n",
    "\n",
    "\n",
    "possible_toxic_words = show_most_informative_features_in_listtest(model,1000)\n",
    "print(possible_toxic_words)\n",
    "\n",
    "\n",
    "t5wset = set(possible_toxic_words) # use this code to delete the duplicated words in the each group's possible toxic words\n",
    "# Then we can get t5wset, t0510set, t1015set, ..., t4550set. Ten groups of possible_toxic_words. \n",
    "\n",
    "# Since it will take about 100 minutes to get each geoup of possible_toxic_words, it is hard to get all 10 groups at\n",
    "# one run. Then we need to output each group of possible_toxic_words in excel form for convenince.\n",
    "t5w = pd.DataFrame.from_dict({\n",
    "    'Toxic_word': list(t5wset)\n",
    "})\n",
    "t5w.to_csv('E:/GW/Textbook/Natural Language Processing/project/t5w.csv', index=False)\n",
    "# Then we can get t5w, t0510, t1015, ..., t4550. Ten groups of possible_toxic_words with excel form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####  step 3 Find the best boundary based on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12524\n",
      "12524\n"
     ]
    }
   ],
   "source": [
    "# Process the validation set.\n",
    "validation_tokens=[word.lower() for words in validation_set for word in word_tokenize(words[0]) if not isNoise(word)]\n",
    "print(len(validation_tokens))\n",
    "real_validation = []\n",
    "for i in validation_tokens:\n",
    "    real_validation += [i.strip('\\.')]\n",
    "print(len(real_validation))\n",
    "\n",
    "# Load the annotated toxic words\n",
    "validation_reference = pd.read_csv(\"E:/GW/Textbook/Natural Language Processing/project/validation_reference.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('unfit', 10),\n",
       " ('idiot', 10),\n",
       " ('moronic', 10),\n",
       " ('bigoted', 10),\n",
       " ('scumbags', 10),\n",
       " ('dumb', 10),\n",
       " ('useless', 10),\n",
       " ('coward', 10),\n",
       " ('racists', 10),\n",
       " ('morons', 10),\n",
       " ('woman', 10),\n",
       " ('uneducated', 10),\n",
       " ('liars', 10),\n",
       " ('traitor', 10),\n",
       " ('disgusting', 10),\n",
       " ('garbage', 10),\n",
       " ('shit', 10),\n",
       " ('hypocritical', 10),\n",
       " ('hypocrisy', 10),\n",
       " ('despicable', 10)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read 10 groups of possible_toxic_words\n",
    "dir_base = \"E:/GW/Textbook/Natural Language Processing/project/bayes/\"\n",
    "\n",
    "def read_file(filename):\n",
    "    input_file_text = read_csv(filename)\n",
    "    return input_file_text\n",
    "   \n",
    "def read_directory_files(directory):\n",
    "    file_texts = []\n",
    "    allwords = []\n",
    "    files = [f for f in listdir(directory) if isfile(join(directory, f))]\n",
    "    for f in files:\n",
    "        file_texts = read_file(join(directory, f))\n",
    "        for row in file_texts.itertuples():\n",
    "            allwords += [(getattr(row, 'Toxic_word'))] \n",
    "        #file_texts += list(file_text)\n",
    "    return allwords\n",
    "    \n",
    "ten_groups_possible_toxic_words = read_directory_files(dir_base) # Get 10 groups of posiible toxic words\n",
    "\n",
    "# Calculate the frequency of each words in ten_groups_possible_toxic_words\n",
    "freq_ten_groups_possible_toxic_words = nltk.FreqDist(ten_groups_possible_toxic_words)\n",
    "\n",
    "# Show the most_common 20 words\n",
    "freq_ten_groups_possible_toxic_words.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function to calculate precision and recall\n",
    "def precison_recall(real_toxic,real_evaluation,evaluation_reference):\n",
    "    pre_toxic = []\n",
    "    for i in real_evaluation:\n",
    "        if i in real_toxic:\n",
    "            pre_toxic += [i]\n",
    "    reference = []\n",
    "    for row in evaluation_reference.itertuples():\n",
    "            reference += [(getattr(row, 'TOX')).lower()]\n",
    "    consist = []\n",
    "    for i in reference:\n",
    "        if i in pre_toxic:\n",
    "            consist += [i]\n",
    "    contrast = []\n",
    "    for i in pre_toxic:\n",
    "        if i not in reference:\n",
    "            contrast += [i]\n",
    "    # Precision and recall\n",
    "    Precision = len(consist)/(len(consist)+len(contrast))\n",
    "    Recall = len(consist)/(len(reference))\n",
    "    print(Precision,Recall)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9070796460176991 0.5061728395061729\n"
     ]
    }
   ],
   "source": [
    "# Calculate the precision and recall\n",
    "possible = []\n",
    "for i in set(ten_groups_possible_toxic_words):\n",
    "    if freq_ten_groups_possible_toxic_words[i] >9:# Here we can change the number to compare different boundaries' performance. \n",
    "        possible += [i]\n",
    "real_toxic = []\n",
    "for i in possible:\n",
    "    real_toxic += [(str(i)).strip('\\.')]\n",
    "precison_recall(real_toxic,real_validation,validation_reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 The second approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Step 1: Split the train_set into clean_group and toxic_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500000"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "352824 16276\n"
     ]
    }
   ],
   "source": [
    "clean_group = []\n",
    "toxic_group = []\n",
    "numb1 = 0\n",
    "numb0 = 0\n",
    "for row in train_set.itertuples():\n",
    "    if getattr(row, 'target') >=0.7:\n",
    "        numb1 +=1 \n",
    "        toxic_group += [(getattr(row, 'comment_text'), math.ceil(getattr(row, 'target')))]\n",
    "for row in train_set.itertuples(): \n",
    "    if getattr(row, 'target') ==0:\n",
    "        clean_group += [(getattr(row, 'comment_text'), math.floor(getattr(row, 'target')))]\n",
    "        numb0 +=1\n",
    "        \n",
    "print(len(clean_group),len(toxic_group))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Step 2: Tokenize clean_group and T_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9572373"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "clean_tokens=[word.lower() for words in clean_group for word in word_tokenize(words[0]) if not isNoise(word)]\n",
    "len(clean_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "365845"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T_tokens=[word.lower() for words in toxic_group for word in word_tokenize(words[0]) if not isNoise(word)]\n",
    "len(T_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Step 3: Calculate the ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('this', 4950),\n",
       " ('people', 3071),\n",
       " ('trump', 3015),\n",
       " ('was', 2971),\n",
       " ('stupid', 2741),\n",
       " ('like', 2695),\n",
       " ('his', 2577),\n",
       " ('has', 2107),\n",
       " ('would', 1966),\n",
       " ('one', 1770),\n",
       " ('get', 1766),\n",
       " ('because', 1384),\n",
       " ('think', 1240),\n",
       " ('only', 1204),\n",
       " ('know', 1200),\n",
       " ('does', 1171),\n",
       " ('idiot', 1156),\n",
       " ('us', 1153),\n",
       " ('time', 1074),\n",
       " ('any', 1063)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_clean_tokens = nltk.FreqDist(clean_tokens)\n",
    "freq_T_tokens = nltk.FreqDist(T_tokens)\n",
    "freq_T_tokens.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic = []\n",
    "uniq = []\n",
    "for i in set(T_tokens):\n",
    "    if freq_clean_tokens[i] ==0:\n",
    "        uniq += [i]\n",
    "        if freq_T_tokens[i] >3:\n",
    "            if (((freq_T_tokens[i]/len(T_tokens))*((numb1)/(numb1+numb0)))/(((freq_clean_tokens[i]+0.1)/len(clean_tokens))*((numb0)/(numb1+numb0))))>1.3:\n",
    "                toxic += [i]\n",
    "    elif freq_clean_tokens[i] !=0:\n",
    "        if (((freq_T_tokens[i]/len(T_tokens))*((numb1)/(numb1+numb0)))/(freq_clean_tokens[i]/len(clean_tokens))*((numb0)/(numb1+numb0)))>1.3:# The threshold here can be adjusted to find the best one\n",
    "            toxic += [i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.955719557195572 0.6395061728395062\n"
     ]
    }
   ],
   "source": [
    "# Calculate the precision and recall\n",
    "real_toxic = []\n",
    "for i in toxic:\n",
    "    real_toxic += [(str(i)).strip('\\.')]\n",
    "precison_recall(real_toxic,real_validation,validation_reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test annotated toxic words\n",
    "test_reference = pd.read_csv(\"E:/GW/Textbook/Natural Language Processing/project/test_reference.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11385\n"
     ]
    }
   ],
   "source": [
    "# Process the test_set\n",
    "test_tokens=[word.lower() for words in test_set for word in word_tokenize(words[0]) if not isNoise(word)]\n",
    "print(len(test_tokens))\n",
    "real_test = []\n",
    "for i in test_tokens:\n",
    "    real_test += [i.strip('\\.')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9094827586206896 0.5877437325905293\n"
     ]
    }
   ],
   "source": [
    "# First approach\n",
    "possible = []\n",
    "for i in set(ten_groups_possible_toxic_words):\n",
    "    if freq_ten_groups_possible_toxic_words[i] >9:# Here we can change the number to compare different boundaries' performance. \n",
    "        possible += [i]\n",
    "first_real_toxic = []\n",
    "for i in possible:\n",
    "    first_real_toxic += [(str(i)).strip('\\.')]\n",
    "precison_recall(first_real_toxic,real_test,test_reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9148936170212766 0.5988857938718662\n"
     ]
    }
   ],
   "source": [
    "# Second approach\n",
    "second_real_toxic = []\n",
    "for i in toxic:\n",
    "    second_real_toxic += [(str(i)).strip('\\.')]\n",
    "precison_recall(second_real_toxic,real_test,test_reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 All toxic words of whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58674\n",
      "1264764\n"
     ]
    }
   ],
   "source": [
    "W_toxic_group = []\n",
    "W_clean_group = []\n",
    "numb1 = 0\n",
    "numb0 = 0\n",
    "for row in whole_data.itertuples():\n",
    "    if getattr(row, 'target') >=0.7:\n",
    "        numb1 +=1 \n",
    "        W_toxic_group += [(getattr(row, 'comment_text'), math.ceil(getattr(row, 'target')))]\n",
    "for row in whole_data.itertuples(): \n",
    "    if getattr(row, 'target') ==0:\n",
    "        W_clean_group += [(getattr(row, 'comment_text'), math.floor(getattr(row, 'target')))]\n",
    "        numb0 +=1\n",
    "print(numb1)\n",
    "print(numb0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33469379\n",
      "1297829\n",
      "673\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['dullards',\n",
       " 'arses',\n",
       " 'fucker',\n",
       " 'scums',\n",
       " 'agian',\n",
       " '*everywhere*',\n",
       " 'scumbags',\n",
       " 'laguillotine',\n",
       " 'batsh*t',\n",
       " 'lilly-white',\n",
       " 'horseshit',\n",
       " 'wavetaker',\n",
       " 'loudmouthed',\n",
       " 'p***y',\n",
       " 'hahahahahahahah',\n",
       " 'prohibtion',\n",
       " 'pussies',\n",
       " 'oafish',\n",
       " 'cow-pie',\n",
       " 'alah',\n",
       " '*please*',\n",
       " 'prohibes',\n",
       " 'sh*t.',\n",
       " 'jinxed',\n",
       " 'slags',\n",
       " 'builts',\n",
       " 'stoopid',\n",
       " 'clitoris',\n",
       " 'chickenshit',\n",
       " 'con-artist',\n",
       " 'slut',\n",
       " 'enviro-zealots',\n",
       " 'waaah',\n",
       " 'non-comment',\n",
       " 'tiny-minded',\n",
       " 'boy-child',\n",
       " 'tard',\n",
       " 'self-loving',\n",
       " '-h.l',\n",
       " 'cking',\n",
       " 'lannatullahi',\n",
       " 'cuck',\n",
       " 'pussy-grabbing',\n",
       " 'jerkoff',\n",
       " 'damnable',\n",
       " 'magaphant',\n",
       " 'dipshit',\n",
       " 'shimmy',\n",
       " 'imbecilic',\n",
       " 'dork',\n",
       " 'presudent',\n",
       " 'trumpaloons',\n",
       " 'ribald',\n",
       " 'troll',\n",
       " 'libatards',\n",
       " 'ill-spirited',\n",
       " 'trudaeu',\n",
       " 'fing',\n",
       " 'inth',\n",
       " 'beautiful.',\n",
       " 'drug-alcohol',\n",
       " 'whorehouse',\n",
       " 'stupid..',\n",
       " 'half-witted',\n",
       " 'f***',\n",
       " 'non-readers',\n",
       " 'muslum',\n",
       " 'supremicists',\n",
       " 'dums',\n",
       " 'nasionaal',\n",
       " 'lhao',\n",
       " 'dickhead',\n",
       " 'lgbtx',\n",
       " 'troll..',\n",
       " 'lesbo',\n",
       " 'krooked',\n",
       " 'scrotum',\n",
       " 'kowalke',\n",
       " 'pathological',\n",
       " 'dead-tree',\n",
       " 'druglords',\n",
       " 'hetro',\n",
       " 'bitches',\n",
       " 'marijane',\n",
       " 'f******',\n",
       " 'hahahahahha',\n",
       " 'galactically',\n",
       " 'unafordable',\n",
       " 'non-word',\n",
       " 'ridiculous..',\n",
       " 'testicles',\n",
       " 'pussy',\n",
       " 'flagellating',\n",
       " 'stupid',\n",
       " 'pu**y',\n",
       " 'damnit',\n",
       " 'boors',\n",
       " 'ctrl-v',\n",
       " 'moronic',\n",
       " 'appologist',\n",
       " 'sub-literate',\n",
       " 'retards',\n",
       " 'libidiots',\n",
       " 'mierda',\n",
       " 'halfwit',\n",
       " 'crotch-grabbing',\n",
       " 'hill-larious',\n",
       " 'non-solutions',\n",
       " 'hatemonger',\n",
       " 'stupid.',\n",
       " 'petrofascist',\n",
       " 'liar',\n",
       " 'muslimaniacs',\n",
       " 'moron.',\n",
       " 'truculent',\n",
       " 'saxberg',\n",
       " 'ingorant',\n",
       " 'narcisist',\n",
       " 'black-face',\n",
       " 'hurted',\n",
       " 'tetched',\n",
       " 'boobs',\n",
       " 'kafirs',\n",
       " 'mysogynist',\n",
       " 'rustlers',\n",
       " 'arsh',\n",
       " 'kid.',\n",
       " 'spray-tanned',\n",
       " 'bloodsucker',\n",
       " 'merika',\n",
       " 'asses',\n",
       " 'rhohypnol',\n",
       " 'beeotch',\n",
       " 'bastard',\n",
       " 'as*hole',\n",
       " 'four-square',\n",
       " 'jerk',\n",
       " 'dumi',\n",
       " 'vaginas',\n",
       " 'clowney',\n",
       " 'friggin',\n",
       " 'europhobia',\n",
       " 'tgat',\n",
       " 'f**king',\n",
       " 'narcissist',\n",
       " 'avvo',\n",
       " 'coward',\n",
       " 'dumbest',\n",
       " 'clowns',\n",
       " 'clown-car',\n",
       " 'imbecile',\n",
       " 'goofier',\n",
       " 'makan',\n",
       " 'shithole',\n",
       " 'kkkanada',\n",
       " 'cunt',\n",
       " 'lmaooo',\n",
       " 'loosers',\n",
       " 'numbskull',\n",
       " 'exulting',\n",
       " 'idiotic',\n",
       " 'twat',\n",
       " 'idiot..',\n",
       " 'live-up',\n",
       " 'bubble.',\n",
       " 'commie-loving',\n",
       " 'hyprocrite',\n",
       " 'racialists',\n",
       " 'foul-mouthed',\n",
       " 'americanophobia',\n",
       " 'feeble-minded',\n",
       " 'faggot',\n",
       " 'swampiest',\n",
       " 'dunderhead',\n",
       " 'f**k',\n",
       " 'humpty-dumpty',\n",
       " 'pantywaist',\n",
       " 'hijo',\n",
       " 'zanele',\n",
       " 'imbecility',\n",
       " 'treasonist',\n",
       " 'dumba**',\n",
       " 'vagina',\n",
       " 'cretin',\n",
       " 'reprters',\n",
       " 'dumbasses',\n",
       " 'f*cking',\n",
       " 'suck-up',\n",
       " 'asshole',\n",
       " 'wheel-to-wheel',\n",
       " 'judeophobia',\n",
       " 'uppitys',\n",
       " 'd*ck',\n",
       " 'masturbating',\n",
       " 'piggies',\n",
       " 'incompetant',\n",
       " 'f**ked',\n",
       " 'bastards',\n",
       " 'as*',\n",
       " 'd**n',\n",
       " 'mid-school',\n",
       " 'gunga-din',\n",
       " 'trumptards',\n",
       " '-g',\n",
       " 'limpbaugh',\n",
       " 'dum-dum',\n",
       " 'stooooopid',\n",
       " 'p**sy',\n",
       " 'wanker',\n",
       " 'catfished',\n",
       " 'duben',\n",
       " 'bitch',\n",
       " '****hole',\n",
       " 'idiots',\n",
       " 'sing-song',\n",
       " 's**t',\n",
       " 'odummy',\n",
       " 'shite',\n",
       " 'hatriotism',\n",
       " 'idiocy',\n",
       " 'unsalted',\n",
       " 'fkn',\n",
       " 'feminazi',\n",
       " 'allies.',\n",
       " 'white-hating',\n",
       " 'sissy',\n",
       " 'scuzzball',\n",
       " 'khogali',\n",
       " 'niggers',\n",
       " 'zedeh',\n",
       " 'unredemptive',\n",
       " 'whores',\n",
       " 'backstabbers',\n",
       " 'transexualism',\n",
       " 'loser..',\n",
       " 'ignorants',\n",
       " 'anti-moslem',\n",
       " 'kafir',\n",
       " 'ocrumba',\n",
       " 'blak',\n",
       " 'dammit',\n",
       " 'bouhlel',\n",
       " 'non-think',\n",
       " 'moron',\n",
       " 'sh*t',\n",
       " 'pino',\n",
       " 'piddled',\n",
       " 'shitting',\n",
       " 'floatilla',\n",
       " 'loud-mouth',\n",
       " 'bestselling',\n",
       " 'wankers',\n",
       " 'fucked',\n",
       " 'fking',\n",
       " 'slanty',\n",
       " 'bullfeathers',\n",
       " 'stupidity..',\n",
       " 'retard',\n",
       " 'stupiditing',\n",
       " 'de-funded',\n",
       " 'tun-tun',\n",
       " 'hecker',\n",
       " 'clown',\n",
       " 'mouth-breathing',\n",
       " 'fox-news-watching',\n",
       " 'buii',\n",
       " '*because',\n",
       " 'shitty',\n",
       " 'whore',\n",
       " 'scumbag',\n",
       " 'trumphile',\n",
       " 'bullshitters',\n",
       " 'scum',\n",
       " 'ass.',\n",
       " 'biglier',\n",
       " 're-board',\n",
       " 'retarded',\n",
       " 'ejaculation',\n",
       " 'hypocrite',\n",
       " 'b*',\n",
       " 'nigga',\n",
       " 'nipples',\n",
       " 'low-trust',\n",
       " 'seriosuly',\n",
       " 'sluts',\n",
       " 'ambidextrous',\n",
       " 'fornicators',\n",
       " 'mysogonistic',\n",
       " 'sadist',\n",
       " 'rga',\n",
       " 'paraphase',\n",
       " 'bullshit',\n",
       " 'fck',\n",
       " 'numb-nuts',\n",
       " 'merrymen',\n",
       " 'b*tch',\n",
       " 'trannies',\n",
       " 'bello',\n",
       " 'dicks',\n",
       " 'small-kid',\n",
       " 'fakey',\n",
       " 'du*mb',\n",
       " 'dull-witted',\n",
       " 'spendaholics',\n",
       " 'soeiro',\n",
       " 'greggs',\n",
       " 'nava',\n",
       " 'lunatics',\n",
       " 'tty',\n",
       " 'sorceress',\n",
       " 'dissembler',\n",
       " 'ass',\n",
       " 'run-over',\n",
       " 'disgusting',\n",
       " 'jeremiad',\n",
       " 'asshat',\n",
       " 'niggardly',\n",
       " 'malignant',\n",
       " 'fuckin',\n",
       " 'caca',\n",
       " 'durkin',\n",
       " 'dumbness',\n",
       " 'no-class',\n",
       " 'minburn',\n",
       " 'f*',\n",
       " 'pynchon',\n",
       " 'he__',\n",
       " 'sblarney',\n",
       " 'candy-ass',\n",
       " 'blowingly',\n",
       " 'bwahahahahahahaha',\n",
       " 'rideing',\n",
       " 'wison',\n",
       " 'imbeciles',\n",
       " 'frickin',\n",
       " 'democraceaophobia',\n",
       " 'sh-t.',\n",
       " 'leftard',\n",
       " 'racist-inspired',\n",
       " 'arsehole',\n",
       " 'fags',\n",
       " 'trumptard',\n",
       " 'adorned',\n",
       " 'blankety',\n",
       " 'hag',\n",
       " 'hypocrit',\n",
       " 'crap',\n",
       " 'brightfart',\n",
       " 'magaphants',\n",
       " 'snot-nosed',\n",
       " 'issueing',\n",
       " 'empathizers',\n",
       " 'tongue-tied',\n",
       " 'stupider',\n",
       " 'wernt',\n",
       " 'dirtbag',\n",
       " 'ignorance..',\n",
       " 'jacka**',\n",
       " 'becos',\n",
       " 'penises',\n",
       " 'buttholes',\n",
       " 'trumpelthinskin',\n",
       " 'krap',\n",
       " 'bigots.',\n",
       " 'fire-bombed',\n",
       " 'stupidist',\n",
       " 'fk',\n",
       " 'trumpnuts',\n",
       " 'chauhan',\n",
       " 'scumbucket',\n",
       " 'fornicator',\n",
       " 'f*ck',\n",
       " 'gooaid',\n",
       " 'buttsore',\n",
       " 'smuck',\n",
       " 'lame-brained',\n",
       " 'christophbia',\n",
       " 'cucks',\n",
       " 'buffoonish',\n",
       " 'goddamn',\n",
       " 'repukes',\n",
       " 'guvnor',\n",
       " 'denuclearized',\n",
       " 'spewer',\n",
       " 'moron-eau',\n",
       " 'jkf',\n",
       " 'morons',\n",
       " 'fucktard',\n",
       " 'mouthbreathers',\n",
       " 'obimbo',\n",
       " 'corkers',\n",
       " 'cheeto-head',\n",
       " 'reichwing',\n",
       " 'b****',\n",
       " 'bullshiat',\n",
       " 'hoodad',\n",
       " 'kookaid',\n",
       " 'homos',\n",
       " 'folla',\n",
       " 'dunderheads',\n",
       " 'dtrumpo',\n",
       " 'h.l',\n",
       " 'moron-in-chief',\n",
       " 'glassy-eyed',\n",
       " 'stf',\n",
       " 'bully-boy',\n",
       " 'trump-humpers',\n",
       " 'sitings',\n",
       " 'straght',\n",
       " 'hypocritic',\n",
       " 'crybaby',\n",
       " 'ahole',\n",
       " 'suckees',\n",
       " 'bugger',\n",
       " 'x-axis',\n",
       " 'aleister',\n",
       " 'pedophilic',\n",
       " 'pendejo',\n",
       " 'brutalizes',\n",
       " 'jew-hating',\n",
       " 'misogamy',\n",
       " 'lickings',\n",
       " 'semi-ignorant',\n",
       " 'kisser',\n",
       " 'f.cking',\n",
       " 'jackass',\n",
       " 'pro-black',\n",
       " 'self-fellating',\n",
       " 'rapey',\n",
       " 'f**ks',\n",
       " 'assholes',\n",
       " 'karmas',\n",
       " 'shoulder-shrugging',\n",
       " 'sh**',\n",
       " 'whitefragilitycankissmyass',\n",
       " 'junk-science',\n",
       " 'greazy',\n",
       " 'hypocrits',\n",
       " 'ignorant',\n",
       " 'lunatic',\n",
       " 'idiocies',\n",
       " 'loud-mouthed',\n",
       " 'snitchin',\n",
       " 'bullsh*t',\n",
       " 'schmucks',\n",
       " 'stu*pid',\n",
       " 'widowmakers',\n",
       " 'a**es',\n",
       " 'buffoons',\n",
       " 'nativegal',\n",
       " 'repugnants',\n",
       " 'fag',\n",
       " 'donutholes',\n",
       " 'greedheads',\n",
       " 'blowjobs',\n",
       " 'sty',\n",
       " 'mysoginist',\n",
       " 'theron',\n",
       " 'witlessly',\n",
       " 'ofass',\n",
       " 'azzed',\n",
       " 'idiot',\n",
       " 'wtf',\n",
       " 'sissies',\n",
       " 'alredy',\n",
       " 'go-daddy',\n",
       " 'a**clown',\n",
       " 'tryna',\n",
       " 'pussycat',\n",
       " 'coloreds',\n",
       " 'masturbate',\n",
       " 'dummocrats',\n",
       " 'p*ssy',\n",
       " 'pooks',\n",
       " 'trumpler',\n",
       " 'sexless',\n",
       " 'sickeningly',\n",
       " 'pocohantas',\n",
       " 'dondi',\n",
       " 'kukae',\n",
       " 'dumpacraps',\n",
       " 'skank',\n",
       " 'crap..',\n",
       " 'stupidty',\n",
       " 'a**hole',\n",
       " 'pasties',\n",
       " 'draft-dodging',\n",
       " 'rule-breaking',\n",
       " 'idiot-in-chief',\n",
       " 'idiots..',\n",
       " 'bat-shit',\n",
       " 'a*s',\n",
       " 'ostupida',\n",
       " 'jack***',\n",
       " 'buncombe',\n",
       " 'wopping',\n",
       " 'nazi-sympathizing',\n",
       " 'cunts',\n",
       " 'sh*',\n",
       " 'draggin',\n",
       " 'p****',\n",
       " 'moochi',\n",
       " 'sore-loser',\n",
       " 'stupid-duty',\n",
       " 'jackasses',\n",
       " 'magadeplorables',\n",
       " 'penis',\n",
       " 'x-man',\n",
       " 'waaaah',\n",
       " 'lameness',\n",
       " 'korrupt',\n",
       " 'pussys',\n",
       " 'athorney',\n",
       " 'facebook-feed',\n",
       " 'warlock',\n",
       " 'white-guilt',\n",
       " 'tlhc',\n",
       " 'gorhan',\n",
       " 'stupidness',\n",
       " 'stupidest',\n",
       " 'cretinous',\n",
       " 'jerks',\n",
       " 'nya',\n",
       " 'terds',\n",
       " 'sucker',\n",
       " 'beckel',\n",
       " 'sorrier',\n",
       " '*those',\n",
       " 'pro-cannabis',\n",
       " 'obungo',\n",
       " 'dunce',\n",
       " 'hi-point',\n",
       " 'harth',\n",
       " 'pound-foolish',\n",
       " 'anal',\n",
       " 'idslaps',\n",
       " 'm.i',\n",
       " 'fool',\n",
       " 'bitchy',\n",
       " 'fuck',\n",
       " 'retentive',\n",
       " 'gun-slinging',\n",
       " 'hairdos',\n",
       " 'nigers',\n",
       " 'bulls**t',\n",
       " 'shit',\n",
       " 'douchebags',\n",
       " 'sucede',\n",
       " 'corupt',\n",
       " 'cat-nutter',\n",
       " 'margy',\n",
       " 'delrahim',\n",
       " 'notalent',\n",
       " 'golfinguy',\n",
       " 'grabbin',\n",
       " 'f***ing',\n",
       " 'douchebag',\n",
       " 'stupidity',\n",
       " 'halfwits',\n",
       " 'jesters',\n",
       " 'face-down',\n",
       " 'stupids',\n",
       " 'bud..',\n",
       " 'pathetic',\n",
       " 'vaginal',\n",
       " 'damn',\n",
       " 'accounts..',\n",
       " 'negros',\n",
       " 'loser',\n",
       " 'unsegregated',\n",
       " 'hypocrites',\n",
       " 'pig-headed',\n",
       " 'savants',\n",
       " 'traitor',\n",
       " 'faggots',\n",
       " 'dimwit',\n",
       " 'hypocritcal',\n",
       " 'fools..',\n",
       " '*trump',\n",
       " 'goofball',\n",
       " 'hate-spewing',\n",
       " 'dim-witted',\n",
       " 'tits',\n",
       " 'ctrl-c',\n",
       " 'fornicate',\n",
       " 'pervy',\n",
       " 'mooslims',\n",
       " 'shlt',\n",
       " 'fricken',\n",
       " 'democrap',\n",
       " 'felonius',\n",
       " 'cowards',\n",
       " 'stupidy',\n",
       " 'magadotards',\n",
       " 'butt..',\n",
       " 'liary',\n",
       " 'jerkiness',\n",
       " 'klausutis',\n",
       " 'mofee',\n",
       " 'nigger',\n",
       " 'tilikum',\n",
       " 'miny',\n",
       " 'culo',\n",
       " 'yalls',\n",
       " 'squads™',\n",
       " 'arse',\n",
       " 'dumb',\n",
       " 'putler',\n",
       " 'colluder',\n",
       " 'baby-in-chief',\n",
       " 'feminazis',\n",
       " 'loathsome',\n",
       " 'nitwit',\n",
       " 'none-too-favorable',\n",
       " 'dumbass',\n",
       " 'dearborne',\n",
       " 'sors',\n",
       " 'controversially-employed',\n",
       " '*hit',\n",
       " 'anus',\n",
       " 'buttocks',\n",
       " 'bufoon',\n",
       " 'midgets',\n",
       " 'stupidly',\n",
       " 'air-head',\n",
       " 'blowjob',\n",
       " 'fools',\n",
       " 'buffoon',\n",
       " 'gasbag',\n",
       " 'fucks',\n",
       " 'maggot',\n",
       " 'pussygrabber',\n",
       " 'blockhead',\n",
       " 'wangle',\n",
       " 'billy-bob',\n",
       " 'bullshite',\n",
       " 'f-ck',\n",
       " 'gangbang',\n",
       " 'ba⚽️⚾️s',\n",
       " 'trump-voting',\n",
       " 'hypocrital',\n",
       " 'schmuck',\n",
       " 'f*ggot',\n",
       " 'clealy',\n",
       " 'chillingly',\n",
       " 'fucking',\n",
       " 'dumber',\n",
       " 'blithering',\n",
       " 'critcize',\n",
       " 'cucking',\n",
       " 'charlize',\n",
       " 'ignoramus',\n",
       " 'emmission',\n",
       " 'prioress',\n",
       " 'westneat',\n",
       " 'min-wage',\n",
       " 'idiotically',\n",
       " 'sh__',\n",
       " 'dumb-ass',\n",
       " 'con-man',\n",
       " 'f^ck',\n",
       " 'black-hooded',\n",
       " 'fearfull',\n",
       " 'sheep..',\n",
       " 'praveen',\n",
       " 'trash-talk',\n",
       " 'shits',\n",
       " 'brain-dead',\n",
       " 'cks',\n",
       " 'wire-tap',\n",
       " 'dumbs',\n",
       " 'suk',\n",
       " 'reacquired']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_clean_tokens=[word.lower() for words in W_clean_group for word in word_tokenize(words[0]) if not isNoise(word)]\n",
    "print(len(W_clean_tokens))\n",
    "W_T_tokens=[word.lower() for words in W_toxic_group for word in word_tokenize(words[0]) if not isNoise(word)]\n",
    "print(len(W_T_tokens))\n",
    "\n",
    "freq_W_clean_tokens = nltk.FreqDist(W_clean_tokens)\n",
    "freq_W_T_tokens = nltk.FreqDist(W_T_tokens)\n",
    "\n",
    "W_toxic = []\n",
    "W_uniq = []\n",
    "for i in set(W_T_tokens):\n",
    "    if freq_W_clean_tokens[i] ==0:\n",
    "        W_uniq += [i]\n",
    "        if freq_W_T_tokens[i] >3:\n",
    "            if (((freq_W_T_tokens[i]/len(W_T_tokens))*((numb1)/(numb1+numb0)))/(((freq_W_clean_tokens[i]+0.1)/len(W_clean_tokens))*((numb0)/(numb1+numb0))))>1.3:\n",
    "                W_toxic += [i]\n",
    "    elif freq_W_clean_tokens[i] !=0:\n",
    "        if (((freq_W_T_tokens[i]/len(W_T_tokens))*((numb1)/(numb1+numb0)))/(freq_W_clean_tokens[i]/len(W_clean_tokens))*((numb0)/(numb1+numb0)))>1.3:# The threshold here can be adjusted to find the best one\n",
    "            W_toxic += [i]\n",
    "print(len(W_toxic))\n",
    "W_toxic"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
